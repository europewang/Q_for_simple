"""
æ•°æ®ç®¡ç†å™¨ - è´Ÿè´£Kçº¿æ•°æ®çš„ç¼“å­˜å’Œè·å–
"""
import os
import pickle
import hashlib
import pandas as pd
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from binance.client import Client
from dotenv import load_dotenv


class DataManager:
    """æ•°æ®ç®¡ç†å™¨ç±»ï¼Œè´Ÿè´£Kçº¿æ•°æ®çš„ç¼“å­˜å’Œè·å–"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨"""
        self.cache_dir = "temp/data_cache"
        self.client = None
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        
        # åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯
        self._init_binance_client()
    
    def _init_binance_client(self):
        """åˆå§‹åŒ–Binance APIå®¢æˆ·ç«¯"""
        load_dotenv()
        api_key = os.getenv('API_KEY')
        api_secret = os.getenv('API_SECRET')
        
        # ä»£ç†è®¾ç½®
        proxies = {}
        if os.getenv('HTTP_PROXY'):
            proxies['http'] = os.getenv('HTTP_PROXY')
        if os.getenv('HTTPS_PROXY'):
            proxies['https'] = os.getenv('HTTPS_PROXY')
        
        try:
            if proxies:
                self.client = Client(api_key, api_secret, requests_params={'proxies': proxies, 'timeout': 30})
            else:
                self.client = Client(api_key, api_secret, requests_params={'timeout': 30})
            
            # æµ‹è¯•è¿æ¥
            self.client.ping()
            print("âœ“ DataManager: æˆåŠŸè¿æ¥åˆ°Binance API")
        except Exception as e:
            print(f"âœ— DataManager: è¿æ¥Binance APIå¤±è´¥: {e}")
            raise
    
    def _generate_cache_filename(self, symbol, start_date, end_date, interval):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        # ä½¿ç”¨ç¬¦å·ã€æ—¥æœŸå’Œæ—¶é—´é—´éš”ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        cache_key = f"{symbol}_{start_date}_{end_date}_{interval}"
        return f"{cache_key}.pkl"
    
    def _is_cache_valid(self, cache_file_path, max_age_hours=24):
        """æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_file_path):
            return False
        
        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        file_mtime = datetime.fromtimestamp(os.path.getmtime(cache_file_path))
        current_time = datetime.now()
        
        # å¦‚æœæ–‡ä»¶è¶…è¿‡æŒ‡å®šå°æ—¶æ•°ï¼Œè®¤ä¸ºè¿‡æœŸ
        if (current_time - file_mtime).total_seconds() > max_age_hours * 3600:
            return False
        
        return True
    
    def _load_from_cache(self, cache_file_path):
        """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            with open(cache_file_path, 'rb') as f:
                data = pickle.load(f)
            print(f"âœ“ ä»ç¼“å­˜åŠ è½½æ•°æ®: {os.path.basename(cache_file_path)}")
            return data
        except Exception as e:
            print(f"âœ— åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def _save_to_cache(self, data, cache_file_path):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶"""
        try:
            with open(cache_file_path, 'wb') as f:
                pickle.dump(data, f)
            print(f"âœ“ æ•°æ®å·²ç¼“å­˜: {os.path.basename(cache_file_path)}")
        except Exception as e:
            print(f"âœ— ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _fetch_from_binance(self, symbol, start_date, end_date, interval):
        """ä»Binance APIè·å–æ•°æ®"""
        print(f"æ­£åœ¨ä»Binanceè·å– {symbol} {interval} æ•°æ®...")
        
        # æ˜ å°„æ—¶é—´é—´éš”
        interval_map = {
            '1m': Client.KLINE_INTERVAL_1MINUTE,
            '30m': Client.KLINE_INTERVAL_30MINUTE,
            '1h': Client.KLINE_INTERVAL_1HOUR,
            '4h': Client.KLINE_INTERVAL_4HOUR,
            '1d': Client.KLINE_INTERVAL_1DAY
        }
        
        if interval not in interval_map:
            raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´é—´éš”: {interval}")
        
        try:
            klines = self.client.get_historical_klines(
                symbol, interval_map[interval], start_date, end_date
            )
            
            # è½¬æ¢ä¸ºDataFrame
            df = self._klines_to_dataframe(klines)
            print(f"âœ“ ä»Binanceè·å–åˆ° {len(df)} æ ¹ {interval} Kçº¿")
            return df
            
        except Exception as e:
            print(f"âœ— ä»Binanceè·å–æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _klines_to_dataframe(self, klines):
        """å°†Kçº¿æ•°æ®è½¬æ¢ä¸ºDataFrame"""
        df = pd.DataFrame(klines, columns=[
            'open_time', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_asset_volume', 'number_of_trades',
            'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
        ])
        
        # è½¬æ¢æ•°æ®ç±»å‹
        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
        df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = df[col].astype(float)
        
        return df
    
    def _check_local_monthly_data(self, symbol, start_date, end_date, interval):
        """
        æ£€æŸ¥æœ¬åœ°æœˆåº¦æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            interval: æ—¶é—´é—´éš”
            
        Returns:
            pandas.DataFrame or None: å¦‚æœæ‰¾åˆ°æœ¬åœ°æ•°æ®åˆ™è¿”å›åˆå¹¶åçš„æ•°æ®ï¼Œå¦åˆ™è¿”å›None
        """
        monthly_data_dir = "output/monthly_data"
        if not os.path.exists(monthly_data_dir):
            return None
            
        # ç”Ÿæˆæœˆä»½èŒƒå›´
        monthly_ranges = self._generate_monthly_ranges(start_date, end_date)
        all_data = []
        missing_months = []
        
        for month_range in monthly_ranges:
            # æ„å»ºæœˆåº¦æ–‡ä»¶å
            monthly_filename = f"{symbol}_{month_range['month_name']}_{interval}.pkl"
            monthly_file_path = os.path.join(monthly_data_dir, monthly_filename)
            
            if os.path.exists(monthly_file_path):
                try:
                    # åŠ è½½æœ¬åœ°æœˆåº¦æ•°æ®
                    monthly_data = self._load_from_cache(monthly_file_path)
                    if monthly_data is not None and not monthly_data.empty:
                        all_data.append(monthly_data)
                        print(f"âœ“ ä½¿ç”¨æœ¬åœ°æ•°æ®: {monthly_filename}")
                    else:
                        missing_months.append(month_range)
                except Exception as e:
                    print(f"âš  åŠ è½½æœ¬åœ°æ•°æ®å¤±è´¥ {monthly_filename}: {e}")
                    missing_months.append(month_range)
            else:
                missing_months.append(month_range)
        
        # å¦‚æœæœ‰ç¼ºå¤±çš„æœˆä»½ï¼Œè¿”å›Noneè®©ç³»ç»Ÿå»è”ç½‘è·å–
        if missing_months:
            print(f"âš  ç¼ºå¤± {len(missing_months)} ä¸ªæœˆçš„æœ¬åœ°æ•°æ®ï¼Œå°†è”ç½‘è·å–")
            return None
            
        # åˆå¹¶æ‰€æœ‰æœ¬åœ°æ•°æ®
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            # æŒ‰æ—¶é—´æ’åºå¹¶å»é‡
            combined_data = combined_data.sort_values('open_time').drop_duplicates(subset=['open_time'])
            combined_data.reset_index(drop=True, inplace=True)
            
            # æ ¹æ®æ—¥æœŸèŒƒå›´è¿‡æ»¤æ•°æ®
            start_dt = self._parse_date_string(start_date)
            end_dt = self._parse_date_string(end_date)
            
            # è½¬æ¢æ—¶é—´æˆ³ä¸ºdatetime
            combined_data['datetime'] = pd.to_datetime(combined_data['open_time'], unit='ms')
            filtered_data = combined_data[
                (combined_data['datetime'] >= start_dt) & 
                (combined_data['datetime'] <= end_dt)
            ].drop('datetime', axis=1)
            
            print(f"âœ“ æˆåŠŸä½¿ç”¨æœ¬åœ°æ•°æ®ï¼Œå…± {len(filtered_data)} æ¡è®°å½•")
            return filtered_data
            
        return None

    def get_kline_data(self, symbol, start_date, end_date, interval='30m', force_refresh=False):
        """
        è·å–Kçº¿æ•°æ®ï¼ˆä¼˜å…ˆä»æœ¬åœ°æœˆåº¦æ•°æ®è·å–ï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·ï¼Œå¦‚ 'BTCUSDT'
            start_date: å¼€å§‹æ—¥æœŸï¼Œå¦‚ '2024-01-01'
            end_date: ç»“æŸæ—¥æœŸï¼Œå¦‚ '2024-01-31'
            interval: æ—¶é—´é—´éš”ï¼Œæ”¯æŒ '1m', '30m', '1h', '4h', '1d'
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
        Returns:
            pandas.DataFrame: Kçº¿æ•°æ®
        """
        # å¦‚æœä¸å¼ºåˆ¶åˆ·æ–°ï¼Œä¼˜å…ˆæ£€æŸ¥æœ¬åœ°æœˆåº¦æ•°æ®
        if not force_refresh:
            local_data = self._check_local_monthly_data(symbol, start_date, end_date, interval)
            if local_data is not None:
                return local_data
        
        # ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
        cache_filename = self._generate_cache_filename(symbol, start_date, end_date, interval)
        cache_file_path = os.path.join(self.cache_dir, cache_filename)
        
        # å¦‚æœä¸å¼ºåˆ¶åˆ·æ–°ï¼Œå†æ£€æŸ¥æ™®é€šç¼“å­˜
        if not force_refresh and self._is_cache_valid(cache_file_path):
            data = self._load_from_cache(cache_file_path)
            if data is not None:
                print(f"âœ“ ä½¿ç”¨ç¼“å­˜æ•°æ®: {cache_filename}")
                return data

        # ä»Binance APIè·å–æ•°æ®
        print(f"ğŸ“¡ ä»Binance APIè·å–æ•°æ®...")
        data = self._fetch_from_binance(symbol, start_date, end_date, interval)
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_to_cache(data, cache_file_path)
        
        return data
    
    def get_multiple_intervals(self, symbol, start_date, end_date, intervals=['30m', '1m'], force_refresh=False):
        """
        è·å–å¤šä¸ªæ—¶é—´é—´éš”çš„Kçº¿æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            intervals: æ—¶é—´é—´éš”åˆ—è¡¨
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
        Returns:
            dict: åŒ…å«ä¸åŒæ—¶é—´é—´éš”æ•°æ®çš„å­—å…¸
        """
        result = {}
        for interval in intervals:
            result[interval] = self.get_kline_data(symbol, start_date, end_date, interval, force_refresh)
        return result
    
    def _parse_date_string(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        try:
            # å°è¯•è§£æ "1 Jan, 2024" æ ¼å¼
            return datetime.strptime(date_str, "%d %b, %Y")
        except ValueError:
            try:
                # å°è¯•è§£æ "2024-01-01" æ ¼å¼
                return datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                raise ValueError(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
    
    def _generate_monthly_ranges(self, start_date, end_date):
        """ç”Ÿæˆæœˆä»½èŒƒå›´åˆ—è¡¨"""
        start_dt = self._parse_date_string(start_date)
        end_dt = self._parse_date_string(end_date)
        
        monthly_ranges = []
        current_date = start_dt.replace(day=1)  # ä»æœˆåˆå¼€å§‹
        
        while current_date <= end_dt:
            # è®¡ç®—å½“æœˆçš„ç»“æŸæ—¥æœŸ
            next_month = current_date + relativedelta(months=1)
            month_end = next_month - timedelta(days=1)
            
            # å¦‚æœæœˆæœ«è¶…è¿‡äº†ç»“æŸæ—¥æœŸï¼Œä½¿ç”¨ç»“æŸæ—¥æœŸ
            if month_end > end_dt:
                month_end = end_dt
            
            # å¦‚æœå½“å‰æœˆçš„å¼€å§‹æ—¥æœŸå°äºå®é™…å¼€å§‹æ—¥æœŸï¼Œä½¿ç”¨å®é™…å¼€å§‹æ—¥æœŸ
            month_start = max(current_date, start_dt)
            
            monthly_ranges.append({
                'start': month_start.strftime("%d %b, %Y"),
                'end': month_end.strftime("%d %b, %Y"),
                'month_name': current_date.strftime("%Y-%m")
            })
            
            current_date = next_month
        
        return monthly_ranges
    
    def get_kline_data_monthly(self, symbol, start_date, end_date, interval='30m', 
                              force_refresh=False, save_monthly_files=True, monthly_output_dir=None):
        """
        æŒ‰æœˆä»½æ‹†åˆ†è·å–Kçº¿æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            interval: æ—¶é—´é—´éš”
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            save_monthly_files: æ˜¯å¦ä¿å­˜æœˆåº¦æ–‡ä»¶
            monthly_output_dir: æœˆåº¦æ–‡ä»¶è¾“å‡ºç›®å½•
        
        Returns:
            pandas.DataFrame: åˆå¹¶åçš„å®Œæ•´Kçº¿æ•°æ®
        """
        print(f"å¼€å§‹æŒ‰æœˆä»½è·å– {symbol} {interval} æ•°æ®...")
        
        # ç”Ÿæˆæœˆä»½èŒƒå›´
        monthly_ranges = self._generate_monthly_ranges(start_date, end_date)
        print(f"å°†åˆ† {len(monthly_ranges)} ä¸ªæœˆä»½è·å–æ•°æ®")
        
        # åˆ›å»ºæœˆåº¦è¾“å‡ºç›®å½•
        if save_monthly_files and monthly_output_dir:
            if not os.path.exists(monthly_output_dir):
                os.makedirs(monthly_output_dir)
        
        all_data = []
        
        for i, month_range in enumerate(monthly_ranges, 1):
            print(f"\n[{i}/{len(monthly_ranges)}] è·å– {month_range['month_name']} æ•°æ®...")
            print(f"  æ—¶é—´èŒƒå›´: {month_range['start']} è‡³ {month_range['end']}")
            
            try:
                # è·å–å½“æœˆæ•°æ®
                monthly_data = self.get_kline_data(
                    symbol, month_range['start'], month_range['end'], 
                    interval, force_refresh
                )
                
                if not monthly_data.empty:
                    all_data.append(monthly_data)
                    print(f"  âœ“ è·å–åˆ° {len(monthly_data)} æ¡æ•°æ®")
                    
                    # ä¿å­˜æœˆåº¦æ–‡ä»¶
                    if save_monthly_files and monthly_output_dir:
                        monthly_filename = f"{symbol}_{month_range['month_name']}_{interval}.pkl"
                        monthly_file_path = os.path.join(monthly_output_dir, monthly_filename)
                        self._save_to_cache(monthly_data, monthly_file_path)
                        print(f"  âœ“ æœˆåº¦æ•°æ®å·²ä¿å­˜: {monthly_filename}")
                else:
                    print(f"  âš  è¯¥æœˆä»½æ²¡æœ‰æ•°æ®")
                    
            except Exception as e:
                print(f"  âœ— è·å– {month_range['month_name']} æ•°æ®å¤±è´¥: {e}")
                continue
        
        # åˆå¹¶æ‰€æœ‰æœˆä»½çš„æ•°æ®
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            # æŒ‰æ—¶é—´æ’åºå¹¶å»é‡
            combined_data = combined_data.sort_values('open_time').drop_duplicates(subset=['open_time'])
            combined_data.reset_index(drop=True, inplace=True)
            
            print(f"\nâœ“ æ‰€æœ‰æœˆä»½æ•°æ®åˆå¹¶å®Œæˆï¼Œå…± {len(combined_data)} æ¡æ•°æ®")
            return combined_data
        else:
            print("\nâœ— æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
    
    def clear_cache(self, symbol=None, older_than_days=None):
        """
        æ¸…ç†ç¼“å­˜æ–‡ä»¶
        
        Args:
            symbol: æŒ‡å®šæ¸…ç†æŸä¸ªäº¤æ˜“å¯¹çš„ç¼“å­˜ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
            older_than_days: æ¸…ç†å¤šå°‘å¤©å‰çš„ç¼“å­˜ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
        """
        if not os.path.exists(self.cache_dir):
            return
        
        files_removed = 0
        for filename in os.listdir(self.cache_dir):
            if not filename.endswith('.pkl'):
                continue
            
            file_path = os.path.join(self.cache_dir, filename)
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…æŒ‡å®šçš„äº¤æ˜“å¯¹
            if symbol and not filename.startswith(symbol):
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¹´é¾„
            if older_than_days:
                file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                if (datetime.now() - file_mtime).days < older_than_days:
                    continue
            
            try:
                os.remove(file_path)
                files_removed += 1
                print(f"âœ“ åˆ é™¤ç¼“å­˜æ–‡ä»¶: {filename}")
            except Exception as e:
                print(f"âœ— åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        print(f"å…±åˆ é™¤ {files_removed} ä¸ªç¼“å­˜æ–‡ä»¶")
    
    def list_cache_files(self):
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
        if not os.path.exists(self.cache_dir):
            print("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
            return
        
        cache_files = []
        for filename in os.listdir(self.cache_dir):
            if filename.endswith('.pkl'):
                file_path = os.path.join(self.cache_dir, filename)
                file_size = os.path.getsize(file_path)
                file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                cache_files.append({
                    'filename': filename,
                    'size_kb': round(file_size / 1024, 2),
                    'modified_time': file_mtime.strftime('%Y-%m-%d %H:%M:%S')
                })
        
        if cache_files:
            print("\nç¼“å­˜æ–‡ä»¶åˆ—è¡¨:")
            print("-" * 80)
            print(f"{'æ–‡ä»¶å':<40} {'å¤§å°(KB)':<10} {'ä¿®æ”¹æ—¶é—´':<20}")
            print("-" * 80)
            for file_info in cache_files:
                print(f"{file_info['filename']:<40} {file_info['size_kb']:<10} {file_info['modified_time']:<20}")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶")


# åˆ›å»ºå…¨å±€æ•°æ®ç®¡ç†å™¨å®ä¾‹
data_manager = DataManager()